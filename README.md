# **ğŸš€ BigQuery CSV Uploader**  

Easily upload multiple **CSV files** to **Google BigQuery**, ensuring seamless data integration with **automatic table creation, retry mechanisms, and detailed logging**. This tool is built for **data engineers, analysts, and developers** who need a **robust and scalable** solution for managing CSV imports into BigQuery.

---

## **ğŸ”¥ Features**
âœ… **Automated Table Creation** â€“ No need to manually create tables; the script does it for you!  
âœ… **Smart Retry Logic** â€“ Retries failed uploads **up to 3 times** before marking them as failed.  
âœ… **Seamless BigQuery Integration** â€“ Uses the official **Google Cloud BigQuery API**.  
âœ… **Detailed Logging** â€“ Get clear insights into upload progress, errors, and success rates.  
âœ… **Handles Large Datasets** â€“ Supports bulk uploads efficiently.  

---

## **ğŸ“Œ How It Works**
1ï¸âƒ£ Drop your **CSV files** into the script.  
2ï¸âƒ£ It checks if the corresponding tables exist in **BigQuery**.  
3ï¸âƒ£ If missing, it **creates the table** automatically.  
4ï¸âƒ£ The script **uploads the data**, handling any errors or retries.  
5ï¸âƒ£ Get a **summary of failed files** (if any).  

---

## **ğŸ”§ Setup & Usage**
```python
csv_files = {
    "sales_data.csv": open("sales_data.csv", "rb").read(),
    "customer_info.csv": open("customer_info.csv", "rb").read(),
}

failed_files = upload_csv_to_bigquery("your-project-id", "your-dataset-id", csv_files)

if failed_files is not None:
    print("Some files failed to upload:", failed_files)
else:
    print("All files uploaded successfully! ğŸš€")
```

---

## **ğŸ“¦ Why Use This?**
- ğŸ **Fast & Efficient** â€“ Optimized for **high-performance bulk uploads**.  
- ğŸ›  **No Manual Work** â€“ Tables are **auto-created** and **autodetected**.  
- ğŸ”„ **Safe & Reliable** â€“ **Retries failures** to ensure **data integrity**.  

An **ETL pipelines, data warehousing, and real-time analytics** to streamline your **BigQuery CSV uploads**.
